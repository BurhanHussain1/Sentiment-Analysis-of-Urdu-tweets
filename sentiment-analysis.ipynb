{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a31300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80756e8",
   "metadata": {},
   "source": [
    "# Load csv file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9df15cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\BURHAN HUSSAIN\\\\Desktop\\\\nlp\\\\urduSentimentdata.csv\"\n",
    "data = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae212f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0\n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0\n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0\n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1e41c",
   "metadata": {},
   "source": [
    "# Load stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66816267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stopwords Loaded: 719\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = \"C:\\\\Users\\\\BURHAN HUSSAIN\\\\Desktop\\\\nlp\\\\stopwords-ur\"\n",
    "stopwords = set()\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = set([line.strip() for line in file])\n",
    "print(\"Total Stopwords Loaded:\", len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f948",
   "metadata": {},
   "source": [
    "# Remove Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "442afeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "def remove_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"                                  \n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002500-\\U00002BEF\"  # Chinese characters\n",
    "        \"\\U00002702-\\U000027B0\"  # miscellaneous symbols\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U0001F926-\\U0001F937\"\n",
    "        \"\\U00010000-\\U0010FFFF\"  # other unicode symbols\n",
    "        \"\\u2640-\\u2642\"          # gender symbols\n",
    "        \"\\u2600-\\u2B55\"          # miscellaneous symbols and arrows\n",
    "        \"\\u200D\"                 # zero-width joiner\n",
    "        \"\\u23CF\"                 # eject button\n",
    "        \"\\u23E9\"                 # fast forward button\n",
    "        \"\\u231A\"                 # watch symbol\n",
    "        \"\\u3030\"                 # wavy dash\n",
    "        \"\\uFE0F\"                 # variation selector\n",
    "        \"\\u2069\"                 # close PDI\n",
    "        \"\\u2066\"                 # open FSI\n",
    "        \"\\u200C-\\u200D\"          # ZWNJ and ZWJ\n",
    "        \"]+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f6ef31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cfcc1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ ØªØ³ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic\n",
       "0   ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº ...           1.0\n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0\n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0\n",
       "3                                        Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†            0.0\n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0\n",
       "5          Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº            1.0\n",
       "6        Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ...            0.0\n",
       "7                                Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†           0.0\n",
       "8  ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ ØªØ³ÛŒ...           1.0\n",
       "9              ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’            1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44603f23",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a70b69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "dum = string.punctuation\n",
    "dum += 'Û”Û”' \n",
    "dum += 'Û”'\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', dum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "588fd6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ÛŒÛ Ø§Ø³Ù…Ø¨Ù„ÛŒ Ø¨ØºÛŒØ± ÚˆÛŒØ²Ù„ Ú©Û’ Ú†Ù„ Ø±ÛÛŒ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û Ø§ÙØ³Ú©Ùˆ Ú¯Ø¯Ú¾Ø§ '' Ú†Ù„Ø§ Ø±ÛØ§ Ú¾Û’ Ú¯Ø¯Ú¾Û’ Ú©Ùˆ ÚˆÛŒØ²Ù„ Ú©ÛŒ Ù†ÛÛŒÚº `` ÚˆÙ†ÚˆÛ’ '' Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’ \""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['urdu_text'][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63b19ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04c7d602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÛŒÛ Ø§Ø³Ù…Ø¨Ù„ÛŒ Ø¨ØºÛŒØ± ÚˆÛŒØ²Ù„ Ú©Û’ Ú†Ù„ Ø±ÛÛŒ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û Ø§ÙØ³Ú©Ùˆ Ú¯Ø¯Ú¾Ø§  Ú†Ù„Ø§ Ø±ÛØ§ Ú¾Û’ Ú¯Ø¯Ú¾Û’ Ú©Ùˆ ÚˆÛŒØ²Ù„ Ú©ÛŒ Ù†ÛÛŒÚº  ÚˆÙ†ÚˆÛ’  Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’ '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['urdu_text'][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e93c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LughaatNLP import LughaatNLP\n",
    "from LughaatNLP import LughaatNLP\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemmatized_sentence = urdu_text_processing.lemmatize_sentence(text)\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b34d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemma_text'] = data['urdu_text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b82f9",
   "metadata": {},
   "source": [
    "# Remove English and Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2c296f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ED(text):\n",
    "    return re.sub(\"[^\\u0600-\\u06FF\\s]\", ' ', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6450438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_ED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf642043",
   "metadata": {},
   "source": [
    "# Remove URL and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82dea9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls_and_hashtags(text):\n",
    "    url_pattern = r'http[s]?://\\S+|www\\.\\S+'  \n",
    "    hashtag_pattern = r'#\\w+' \n",
    "    text_without_urls = re.sub(url_pattern, '', text)\n",
    "    clean_text = re.sub(hashtag_pattern, '', text_without_urls)\n",
    "    \n",
    "    return clean_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f82ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_urls_and_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4dead",
   "metadata": {},
   "source": [
    "# Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b32c6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fac00840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "761d4616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù¾Ø§Ø¦ÛŒÙ†</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø§ÛŒØ³ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û’ ÛÙˆÛŒØ§ ØªØ³ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†Ø¬ÙˆÙ…ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic\n",
       "0                         Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’           1.0\n",
       "1         Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº           1.0\n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...           0.0\n",
       "3                                              Ù¾Ø§Ø¦ÛŒÙ†           0.0\n",
       "4               Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø§ÛŒØ³ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±           1.0\n",
       "5                            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±           1.0\n",
       "6                               Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±           0.0\n",
       "7                                Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†           0.0\n",
       "8  ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û’ ÛÙˆÛŒØ§ ØªØ³ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†Ø¬ÙˆÙ…ÛŒ...           1.0\n",
       "9                      Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’           1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd318fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_posts(text):\n",
    "    words = text.split()\n",
    "    if len(words) < 3:\n",
    "        return None  # Return None for short posts\n",
    "    else:\n",
    "        return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(filter_short_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd77aed",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e45ad6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f2569a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_text'] = data['urdu_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c5b4bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Ù„ÛŒÙ†Û’, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯Ù†, Ú©ÙˆØ¬ÛŒ, Ú†Ø§ÛÛŒÛ’]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±Ùˆ, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ, Ù†ÙˆÚº, Ø¯Ø³Ø¯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§Ø¦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù¾Ø§Ø¦ÛŒÙ†</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Ù¾Ø§Ø¦ÛŒÙ†]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø§ÛŒØ³ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø¬ÛŒ, Ø§ÛŒØ³, Ø­Ø§Ù…Ø¯, Ù…ÛŒØ±]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic  \\\n",
       "0                         Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’           1.0   \n",
       "1         Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº           1.0   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...           0.0   \n",
       "3                                              Ù¾Ø§Ø¦ÛŒÙ†           0.0   \n",
       "4               Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø§ÛŒØ³ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±           1.0   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0                   [Ù„ÛŒÙ†Û’, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯Ù†, Ú©ÙˆØ¬ÛŒ, Ú†Ø§ÛÛŒÛ’]  \n",
       "1  [Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±Ùˆ, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ, Ù†ÙˆÚº, Ø¯Ø³Ø¯...  \n",
       "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§Ø¦...  \n",
       "3                                            [Ù¾Ø§Ø¦ÛŒÙ†]  \n",
       "4     [Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø¬ÛŒ, Ø§ÛŒØ³, Ø­Ø§Ù…Ø¯, Ù…ÛŒØ±]  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4abe12",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16e79306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  TF-IDF Score\n",
      "1281      Ø§Ù„Ù„Û    206.778755\n",
      "5328        Ø¬ÛŒ    206.190535\n",
      "2319       Ø¨Ø§Øª    198.241904\n",
      "19272       Ú¾Û’    188.142362\n",
      "5747       Ø®Ø§Ù†    186.837846\n",
      "9270      ØµØ§Ø­Ø¨    169.203539\n",
      "8227      Ø³Ù†Ø¯Ú¾    150.378390\n",
      "14964  Ù¾Ø§Ú©Ø³ØªØ§Ù†    133.401037\n",
      "13435     Ù†ÙˆØ§Ø²    126.488377\n",
      "9168     Ø´Ú©Ø±ÛŒÛ    125.973711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['urdu_text'])\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum up the TF-IDF scores for each word across all documents\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "# Create a DataFrame with words and their corresponding TF-IDF scores\n",
    "tfidf_scores_df = pd.DataFrame({'Word': feature_names, 'TF-IDF Score': tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame by TF-IDF score in descending order\n",
    "top_words_df = tfidf_scores_df.sort_values(by='TF-IDF Score', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "print(top_words_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93872d78",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46ecd61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 words similar to 'Ø³ÛŒØ§Ø³ÛŒ':\n",
      "Ú©Ø§Ù…: 0.9986\n",
      "Ø´Ø±ÙˆØ¹: 0.9983\n",
      "Ù„Ú©Ú¾Ø§: 0.9983\n",
      "Ú©Ù„Ø§Ø³: 0.9981\n",
      "Ú©Ù…Ø§Ù„: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "tokenized_reviews = [nltk.word_tokenize(review) for review in data['urdu_text']]\n",
    "\n",
    "model_cbow = Word2Vec(sentences=tokenized_reviews, min_count=1, window=2, sg=0)\n",
    "\n",
    "\n",
    "word_vectors = model_cbow.wv\n",
    "\n",
    "word_vectors_df = pd.DataFrame(word_vectors.vectors, index=word_vectors.index_to_key)\n",
    "\n",
    "similar_words = model_cbow.wv.most_similar('Ø³ÛŒØ§Ø³ÛŒ', topn=5)\n",
    "\n",
    "print(\"\\nTop 5 words similar to 'Ø³ÛŒØ§Ø³ÛŒ':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37aae4",
   "metadata": {},
   "source": [
    "# Biagram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d24702ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Bigrams:\n",
      "Ø¹Ù…Ø±Ø§Ù† Ø®Ø§Ù†: 497\n",
      "Ù†ÙˆØ§Ø² Ø´Ø±ÛŒÙ: 442\n",
      "Ø³Ù†Ø¯Ú¾ Ù¾ÙˆÙ„ÛŒØ³: 299\n",
      "Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ: 223\n",
      "Ø®Ø§Ù† ØµØ§Ø­Ø¨: 182\n",
      "Ú©ÛŒÙ¾Ù¹Ù† ØµÙØ¯Ø±: 176\n",
      "Ø¬Ø²Ø§Ú© Ø§Ù„Ù„Û: 161\n",
      "Ù…Ø±ÛŒÙ… Ù†ÙˆØ§Ø²: 158\n",
      "Ù† Ù„ÛŒÚ¯: 149\n",
      "ÙØ§Ù„Ùˆ Ø¨ÛŒÚ©: 136\n",
      "\n",
      "Top 10 Trigrams:\n",
      "Ù¾ÛŒ ÚˆÛŒ Ø§ÛŒÙ…: 85\n",
      "ØµÙ„ÛŒ Ø§Ù„Ù„Û Ø¹Ù„ÛŒÛ: 83\n",
      "ÙØ§Ù„Ùˆ ÙØ§Ù„Ùˆ Ø¨ÛŒÚ©: 71\n",
      "Ø¬Ø²Ø§Ú© Ø§Ù„Ù„Û Ø®ÛŒØ±: 68\n",
      "Ø§Ø³ØªØºÙØ± Ø§Ù„Ù„Ù‡ÛÙ ÙˆØ§ØªÙˆØ¨: 53\n",
      "Ø¹Ø·Ø§ ÙØ±Ù…Ø§Ø¦Û’ Ø¢Ù…ÛŒÙ†: 52\n",
      "Ø§Ù„Ù„Ù‡ÛÙ ÙˆØ§ØªÙˆØ¨ Ø§Ù„ÙŠÙ‡ÛÙ: 51\n",
      "Ù…ÛŒÙ†Ø´Ù† Ø±ÛŒÙ¹ÙˆØ¦ÛŒÙ¹ Ø±ÛŒÙ¹ÙˆØ¦ÛŒÙ¹: 48\n",
      "Ø±ÛŒÙ¹ÙˆØ¦ÛŒÙ¹ Ø±ÛŒÙ¹ÙˆØ¦ÛŒÙ¹ ÙØ§Ù„Ùˆ: 48\n",
      "Ø§ÛŒØ³ Ø§ÛŒÚ† Ø¬ÙˆØ§Ø¨: 48\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \" \".join(data['urdu_text'])\n",
    "\n",
    "tokenized_text = nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "bigrams = list(ngrams(tokenized_text, 2))\n",
    "\n",
    "trigrams = list(ngrams(tokenized_text, 3))\n",
    "\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "top_bigrams = bigram_counts.most_common(10)\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, freq in top_bigrams:\n",
    "    print(f\"{' '.join(bigram)}: {freq}\")\n",
    "\n",
    "top_trigrams = trigram_counts.most_common(10)\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "for trigram, freq in top_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e0d5055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.72\n",
      "Recall: 0.87\n",
      "F1-Score: 0.79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['urdu_text'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, data['is_sarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f3eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the sentence 'ÙˆÛ Ù…Ø°Ø§Ù‚ Ú©Ø± Ø±ÛØ§ ØªÚ¾Ø§' is: 1.0\n"
     ]
    }
   ],
   "source": [
    "test = \"ÙˆÛ Ù…Ø°Ø§Ù‚ Ú©Ø± Ø±ÛØ§ ØªÚ¾Ø§\" \n",
    "test_tfidf = tfidf_vectorizer.transform([test]) \n",
    "label = model.predict(test_tfidf)\n",
    "\n",
    "print(f\"The predicted label for the sentence '{test}' is: {label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
