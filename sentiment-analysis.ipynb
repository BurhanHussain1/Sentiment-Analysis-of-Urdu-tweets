{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a31300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80756e8",
   "metadata": {},
   "source": [
    "# Load csv file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9df15cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\BURHAN HUSSAIN\\\\Desktop\\\\nlp\\\\urduSentimentdata.csv\"\n",
    "data = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae212f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...           1.0\n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...           1.0\n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...           0.0\n",
       "3                                       نہیں پائین 😎           0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1e41c",
   "metadata": {},
   "source": [
    "# Load stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66816267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stopwords Loaded: 719\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = \"C:\\\\Users\\\\BURHAN HUSSAIN\\\\Desktop\\\\nlp\\\\stopwords-ur\"\n",
    "stopwords = set()\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = set([line.strip() for line in file])\n",
    "print(\"Total Stopwords Loaded:\", len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f948",
   "metadata": {},
   "source": [
    "# Remove Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "442afeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "def remove_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"                                  \n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002500-\\U00002BEF\"  # Chinese characters\n",
    "        \"\\U00002702-\\U000027B0\"  # miscellaneous symbols\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U0001F926-\\U0001F937\"\n",
    "        \"\\U00010000-\\U0010FFFF\"  # other unicode symbols\n",
    "        \"\\u2640-\\u2642\"          # gender symbols\n",
    "        \"\\u2600-\\u2B55\"          # miscellaneous symbols and arrows\n",
    "        \"\\u200D\"                 # zero-width joiner\n",
    "        \"\\u23CF\"                 # eject button\n",
    "        \"\\u23E9\"                 # fast forward button\n",
    "        \"\\u231A\"                 # watch symbol\n",
    "        \"\\u3030\"                 # wavy dash\n",
    "        \"\\uFE0F\"                 # variation selector\n",
    "        \"\\u2069\"                 # close PDI\n",
    "        \"\\u2066\"                 # open FSI\n",
    "        \"\\u200C-\\u200D\"          # ZWNJ and ZWJ\n",
    "        \"]+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f6ef31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cfcc1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے تسی...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic\n",
       "0   ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں ...           1.0\n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...           1.0\n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...           0.0\n",
       "3                                        نہیں پائین            0.0\n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...           1.0\n",
       "5          قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں            1.0\n",
       "6        انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ...            0.0\n",
       "7                                حامد میر صاحب ویلڈن           0.0\n",
       "8  یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے تسی...           1.0\n",
       "9              یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے            1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44603f23",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a70b69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "dum = string.punctuation\n",
    "dum += '۔۔' \n",
    "dum += '۔'\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', dum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "588fd6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"یہ اسمبلی بغیر ڈیزل کے چل رہی ہے کیونکہ اِسکو گدھا '' چلا رہا ھے گدھے کو ڈیزل کی نہیں `` ڈنڈے '' کی ضرورت ہوتی ہے \""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['urdu_text'][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63b19ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04c7d602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'یہ اسمبلی بغیر ڈیزل کے چل رہی ہے کیونکہ اِسکو گدھا  چلا رہا ھے گدھے کو ڈیزل کی نہیں  ڈنڈے  کی ضرورت ہوتی ہے '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['urdu_text'][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e93c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LughaatNLP import LughaatNLP\n",
    "from LughaatNLP import LughaatNLP\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemmatized_sentence = urdu_text_processing.lemmatize_sentence(text)\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b34d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemma_text'] = data['urdu_text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b82f9",
   "metadata": {},
   "source": [
    "# Remove English and Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2c296f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ED(text):\n",
    "    return re.sub(\"[^\\u0600-\\u06FF\\s]\", ' ', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6450438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_ED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf642043",
   "metadata": {},
   "source": [
    "# Remove URL and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82dea9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls_and_hashtags(text):\n",
    "    url_pattern = r'http[s]?://\\S+|www\\.\\S+'  \n",
    "    hashtag_pattern = r'#\\w+' \n",
    "    text_without_urls = re.sub(url_pattern, '', text)\n",
    "    clean_text = re.sub(hashtag_pattern, '', text_without_urls)\n",
    "    \n",
    "    return clean_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f82ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_urls_and_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4dead",
   "metadata": {},
   "source": [
    "# Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b32c6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fac00840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "761d4616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>لینے شادی فسادن کوجی چاہیے</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں کھانا سرو چڑیل چاچی نوں دسدی آں</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی اپوزیش...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>پائین</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مراد علی شاہ بھیس ڈی جی ایس حامد میر</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار قاتل اعتبار</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں تھکا سوچوں سفر</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے ہویا تسی تے پکے نجومی...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>سمجھتے سارا پاکستان بیوقوف ھے</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic\n",
       "0                         لینے شادی فسادن کوجی چاہیے           1.0\n",
       "1         چل مہمانوں کھانا سرو چڑیل چاچی نوں دسدی آں           1.0\n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی اپوزیش...           0.0\n",
       "3                                              پائین           0.0\n",
       "4               مراد علی شاہ بھیس ڈی جی ایس حامد میر           1.0\n",
       "5                            قابل اعتبار قاتل اعتبار           1.0\n",
       "6                               انساں تھکا سوچوں سفر           0.0\n",
       "7                                حامد میر صاحب ویلڈن           0.0\n",
       "8  یار وچارہ ویلا ہوندا آرے ہویا تسی تے پکے نجومی...           1.0\n",
       "9                      سمجھتے سارا پاکستان بیوقوف ھے           1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd318fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_posts(text):\n",
    "    words = text.split()\n",
    "    if len(words) < 3:\n",
    "        return None  # Return None for short posts\n",
    "    else:\n",
    "        return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['urdu_text'] = data['urdu_text'].apply(filter_short_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd77aed",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e45ad6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f2569a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_text'] = data['urdu_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c5b4bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>لینے شادی فسادن کوجی چاہیے</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[لینے, شادی, فسادن, کوجی, چاہیے]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں کھانا سرو چڑیل چاچی نوں دسدی آں</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[چل, مہمانوں, کھانا, سرو, چڑیل, چاچی, نوں, دسد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی اپوزیش...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[کامران, خان, آپکی, دن, بھریہ, زمہ, داری, لگائ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>پائین</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[پائین]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مراد علی شاہ بھیس ڈی جی ایس حامد میر</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[مراد, علی, شاہ, بھیس, ڈی, جی, ایس, حامد, میر]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic  \\\n",
       "0                         لینے شادی فسادن کوجی چاہیے           1.0   \n",
       "1         چل مہمانوں کھانا سرو چڑیل چاچی نوں دسدی آں           1.0   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی اپوزیش...           0.0   \n",
       "3                                              پائین           0.0   \n",
       "4               مراد علی شاہ بھیس ڈی جی ایس حامد میر           1.0   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0                   [لینے, شادی, فسادن, کوجی, چاہیے]  \n",
       "1  [چل, مہمانوں, کھانا, سرو, چڑیل, چاچی, نوں, دسد...  \n",
       "2  [کامران, خان, آپکی, دن, بھریہ, زمہ, داری, لگائ...  \n",
       "3                                            [پائین]  \n",
       "4     [مراد, علی, شاہ, بھیس, ڈی, جی, ایس, حامد, میر]  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4abe12",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16e79306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  TF-IDF Score\n",
      "1281      اللہ    206.778755\n",
      "5328        جی    206.190535\n",
      "2319       بات    198.241904\n",
      "19272       ھے    188.142362\n",
      "5747       خان    186.837846\n",
      "9270      صاحب    169.203539\n",
      "8227      سندھ    150.378390\n",
      "14964  پاکستان    133.401037\n",
      "13435     نواز    126.488377\n",
      "9168     شکریہ    125.973711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['urdu_text'])\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum up the TF-IDF scores for each word across all documents\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "# Create a DataFrame with words and their corresponding TF-IDF scores\n",
    "tfidf_scores_df = pd.DataFrame({'Word': feature_names, 'TF-IDF Score': tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame by TF-IDF score in descending order\n",
    "top_words_df = tfidf_scores_df.sort_values(by='TF-IDF Score', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "print(top_words_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93872d78",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46ecd61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 words similar to 'سیاسی':\n",
      "کام: 0.9986\n",
      "شروع: 0.9983\n",
      "لکھا: 0.9983\n",
      "کلاس: 0.9981\n",
      "کمال: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "tokenized_reviews = [nltk.word_tokenize(review) for review in data['urdu_text']]\n",
    "\n",
    "model_cbow = Word2Vec(sentences=tokenized_reviews, min_count=1, window=2, sg=0)\n",
    "\n",
    "\n",
    "word_vectors = model_cbow.wv\n",
    "\n",
    "word_vectors_df = pd.DataFrame(word_vectors.vectors, index=word_vectors.index_to_key)\n",
    "\n",
    "similar_words = model_cbow.wv.most_similar('سیاسی', topn=5)\n",
    "\n",
    "print(\"\\nTop 5 words similar to 'سیاسی':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37aae4",
   "metadata": {},
   "source": [
    "# Biagram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d24702ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Bigrams:\n",
      "عمران خان: 497\n",
      "نواز شریف: 442\n",
      "سندھ پولیس: 299\n",
      "آرمی چیف: 223\n",
      "خان صاحب: 182\n",
      "کیپٹن صفدر: 176\n",
      "جزاک اللہ: 161\n",
      "مریم نواز: 158\n",
      "ن لیگ: 149\n",
      "فالو بیک: 136\n",
      "\n",
      "Top 10 Trigrams:\n",
      "پی ڈی ایم: 85\n",
      "صلی اللہ علیہ: 83\n",
      "فالو فالو بیک: 71\n",
      "جزاک اللہ خیر: 68\n",
      "استغفر اللهہَ واتوب: 53\n",
      "عطا فرمائے آمین: 52\n",
      "اللهہَ واتوب اليهہَ: 51\n",
      "مینشن ریٹوئیٹ ریٹوئیٹ: 48\n",
      "ریٹوئیٹ ریٹوئیٹ فالو: 48\n",
      "ایس ایچ جواب: 48\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \" \".join(data['urdu_text'])\n",
    "\n",
    "tokenized_text = nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "bigrams = list(ngrams(tokenized_text, 2))\n",
    "\n",
    "trigrams = list(ngrams(tokenized_text, 3))\n",
    "\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "top_bigrams = bigram_counts.most_common(10)\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, freq in top_bigrams:\n",
    "    print(f\"{' '.join(bigram)}: {freq}\")\n",
    "\n",
    "top_trigrams = trigram_counts.most_common(10)\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "for trigram, freq in top_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e0d5055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.72\n",
      "Recall: 0.87\n",
      "F1-Score: 0.79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['urdu_text'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, data['is_sarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f3eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the sentence 'وہ مذاق کر رہا تھا' is: 1.0\n"
     ]
    }
   ],
   "source": [
    "test = \"وہ مذاق کر رہا تھا\" \n",
    "test_tfidf = tfidf_vectorizer.transform([test]) \n",
    "label = model.predict(test_tfidf)\n",
    "\n",
    "print(f\"The predicted label for the sentence '{test}' is: {label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
